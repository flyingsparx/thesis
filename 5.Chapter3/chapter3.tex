\chapter{Inferring Interestingness}

A method has been introduced, based on Proposition \ref{proposition:1}, for identifying interesting Tweets. However, this method presents a range of various shortcomings and was found to be un-usable for a large proportion of Tweets. In this chapter, the methodology is modified with the aim of improving its performance and increasing the range of use-cases considered. Since the social structure was found to play an important role in propagation, improvements are centred on including network and user features.

A modification representing a larger contribution is made in order to provide an indication of \textit{how} interesting a piece of information is estimated to be, and more about this particular component is discussed in later sections. Motivation for this is based around wanting to rank Tweets by order of relative interestingness and to highlight the Tweets that may be brought forward to receive further attention.

The proposed methodologies build upon the previously-identified differences between a Tweet's raw popularity, as indicated by its retweet count, and how interesting the Tweet actually is to those who read it. It has been shown that making retweet predictions against models trained with a large number of features can be accurate \cite{zhu11}, but in this work the focus is applied more to the Tweets' contents and properties beyond their static features. That is, that when comparing Tweet popularity, then there may be some content, either within the Tweet itself or perhaps in a resource indicated by a URL contained in the Tweet, that makes the Tweet stand out more to its recipients and to cause its readers to be affectively stimulated.

Of course, this brings about the notion of information \textit{relevance}, and the fact that the same Tweet could be very boring or irrelevant to one user, and very interesting to another. In this work focus is applied to \textit{global} (or `average') interest, where interestingness inferences are made for the general case. It is considered that Tweets that are retweeted more than expected within their authors' local networks, relative to the usual retweet count of the authors' other Tweets, are also likely to be of interest to a wider audience, especially since they are now more likely to penetrate through the social graph enough to be received by users in different communities.
 
As such, the focus of the work in this chapter is that of adapting the inference methodology in order to develop a technique for accurately \textit{quantifying} the interestingness of Tweets. This is concerning universal relevance in terms of highlighting interesting Tweets from the noise. In particular, there are two main improvements over the previous methodology to be made;
\begin{itemize}
    \item Improve the method for generating the \textit{expected} retweet count of a Tweet (in both accuracy and in range of application);
    \item Expand the binary retweet interesting inference into a more useful scale in order to support the \textit{ranking} of information by interest.
\end{itemize}

User influence plays a large role in the exhibited difference between popularity and interestingness. The Background chapter illustrated the example of Justin Bieber, whose account, \texttt{@justinbieber}, is one of the most `influential' on Twitter, with nearly 50 million followers at the time of writing. His Tweets receive an average of around 50-120 thousand retweets per Tweet, and they rarely receive fewer than 40,000 retweets. Since an average Twitter user would generally attract a maximum of a few hundred followers, and would normally receive very few, if any, retweets per Tweet. A particularly interesting Tweet from such a user may be retweeted, for example, between 5-20 times. It is apparent that, in the general case, an uninteresting Tweet from an influential user may receive 50,000 retweets, and an exceptionally interesting Tweet from a less-influential user may be retweeted 30 times. Therefore, user influence dictates that this value cannot alone be indicative of Tweet interest.

However, since interestingness \textit{does} have an effect on an a user's individual retweet decision on a particular Tweet, this absolute retweet count can be used as part of the method for generating an interestingness \textit{score} for that Tweet.

In this chapter, the final question (\textbf{RQ4}) from the hypotheses in Section \ref{section:research_questions} is answered in order to show that Tweet global interestingness can be inferred non-semantically with some degree of accuracy. Contributions made as part of this research include an analysis into the performance of machine learning classifiers for the purpose of social network analysis; proven enhancements to the interestingness inference method introduced in Chapter 4; and a thorough analysis into the performance of the methodology is made for the purposes of demonstrating its relative advantages and disadvantages, and how these relate to the social graph analyses considered earlier in this thesis. 


\section{Interestingness through Tweet Scoring}
A scoring scheme is introduced in order to address the notion of interestingness quantification, allowing certain interesting Tweets to be ranked as `more interesting' than other interesting Tweets. This, in itself, is an improvement over the previous method, which allowed only for Tweets to be labelled as `interesting' or `non-interesting'.

As an enhancement to Proposition \ref{proposition:1}'s method of comparing the observed to expected retweet counts, the new scoring technique is based now on the \textit{distance} between the two counts. The general idea and potential use-case for this is that if a score is known for a set of Tweets, then these can be used as a basis for ordering information as part of information retrieval or an information delivery system, where Tweets can be displayed to users in a more useful way than simply chronologically. In this way, interesting Tweets could be brought forward to users who don't follow the source user or a retweeter and thus deliver information to an interested user, yet without him or her having to know about it first.

\begin{myproposition}
\label{proposition:2}
If the positive difference between a Tweet's observed and predicted popularity is proportionately greater than those attributes in a different Tweet, then the first Tweet is proportionately \textit{more} interesting than the second.
\end{myproposition}

Essentially, Proposition \ref{proposition:2} stems from the following scenario. Consider two Tweets, $A$ and $B$, which have the following properties;
\begin{itemize}
    \item $\ec{A} = 3000$ and $\rc{A} = 3010$
    \item $\ec{B} = 5$ and $\rc{B} = 15$
\end{itemize}
Where $\ec{A}$ and $\ec{B}$ represent the expected retweet count of $A$ and $B$ respectively.

In this case, both Tweets would have been flagged as `interesting' under Proposition \ref{proposition:1} (although, in reality, the derived method would not be able to model users who are typically expected to achieve 3,000 retweets). However, it is clear that, despite the \textit{difference} between the counts being equal, Tweet $B$'s observed retweet count is actually much more significantly proportionately greater than what was expected, and is therefore likely to be more significantly interesting.

Since the proportionate difference is the key to this, the interestingness score, $\score{t}$, for Tweet $t$ is simply given by;
\[
\begin{array}{cc}
    \score{t} = \frac{\rc{t}}{\ec{t}}
\end{array}
\]

This provides a positive score where;

\[
\score{t}
\begin{cases}
    > 1		&	\text{indicates } t	\text{ is interesting} \\
    \leq 1	&	\text{indicates } t	\text{ is non-interesting}
\end{cases}
\]
and where $\score{A} > \score{B}$ implies that $A$ is estimated to be more interesting than $B$.

Since this methodology relies on data collection from Twitter in order to obtain the observed retweet counts, it involves extracting a snapshot of the state of the evaluated Tweets at one stage during their lifetime. Since Tweets are not removed over time (unless they are deleted by their author) they can be discovered and retweeted at any time after their composition and posting.

The work in this chapter assumes that the most significant portion of retweet activity for a specific Tweet has already occurred by the time the information on the Tweet has been collected. \citet{kwak10} carried out investigative analyses into various temporal retweet behaviours, and discovered that, on average, a Tweet receives around 75\% of its retweets within the first day of being posted. 50\% of the retweets of a Tweet even take place within the first \textit{hour} of the Tweet being posted. Due to this, and to ensure that the retweet count collected is mostly representative of the Tweet's extrapolated `final' retweet count, only Tweets that had been posted at least one day ago were considered for experimentation.


\section{Further Adaptations of the Inference Methodology}
Limitations with the previous method dictated that predicting a Tweet's expected re\-tweet count could only work under certain restrictions. In particular, that the user must have a small enough local network (in practice, a follower count of more than 500 or so made the method very unsuitable), and that, due to this, Tweets only attracting very few retweets could effectively be simulated. In addition, Section \ref{section:initial_inference_results} demonstrated that the validations of the interestingness inferences were found to be inaccurate in terms of the agreement with human judgments, although this is likely due to a combination of the above issue in providing much less room for error and the fact that the interestingness decision was only binary.

A new method is proposed, derived from Proposition \ref{proposition:2}, for carrying out the prediction for the value of $\ec{t}$. The method involves generating a classifier model capable of producing a base-line expected retweet count for a given Tweet and its relationship with its author. In this case, the classifier would be trained with the Tweet's actual retweet \textit{count} instead of the binary retweet decision used previously, and it would not require the simulations of the user's local network. Many more features regarding the Tweet, and its content, and its author are used to represent the particular user-Tweet information required for generating the predictions.

Since the graph structure clearly has an impact on message propagation, then it was felt that a significant consideration should be made towards including features relating to the interconnection of users, such as follower counts, Tweet rate, and information on a sample of friends and followers. More detail on the features used is provided in later sections.

Experiments to demonstrate the newly-proposed methodology are to follow these steps:
\begin{enumerate}
    \item Collect information on a number of Tweets and their respective authors from Twitter;
    \item Form a dataset, $T$, of Tweets, each with its author's information embedded;
    \item Split $T$ into a training and test set - $\trainset{}$ and $\testset{}$ respectively;
    \item Train a classifier on $t \forall t \in \trainset{}$. This trained model is known as the global model;
    \item Extract features for each $t \in \testset{}$ and classify each against the trained classifier to obtain $\ec{t} \forall t \in \testset{}$;
    \item Calculate $\score{t}$ from the obtained $\ec{t}$ and the known $\rc{t}$. 
\end{enumerate}

This method is immediately more superior as only a very small amount of data is required to be collected from Twitter. This means that inferences on Tweet interestingness could be made on demand\footnote{Not `live' as it relies on time for retweets to have occurred.}.

In addition to the `global' model, a `user' model was proposed to be built for each user being evaluated. This user model would be much smaller, as it would be based only on the features from a history of that user's Tweets, but would be capable of providing a second value for $\ec{t}$ when testing Tweets against it. 

\begin{mydefinition}
    A Tweet, $t$'s, \textbf{expected `global' retweet count}, $\ecg{t}$, is the retweet count predicted when classifying $t$'s Tweet and user features against the trained global model.\\
    A Tweet, $t$'s, \textbf{expected `user' retweet count}, $\ecu{t}$, is the retweet count predicted when classifying $t$'s Tweet and user features against $\aut{t}{O}$'s trained user model.
\end{mydefinition}

With two such values generated through the comparisons of Tweets to both the global and user models, two scores could be generated as a function of the static value of $\rc{t}$.

\begin{mydefinition}
    A Tweet, $t$'s \textbf{global score} is derived from the global model and is denoted as $\gscore{t} = \frac{\rc{t}}{\ecg{t}}$.\\
    A Tweet, $t$'s \textbf{user score} is derived from $\aut{t}{O}$'s user model and is denoted as  $\uscore{t} = \frac{\rc{t}}{\ecu{t}}$.
\end{mydefinition}


%As such, the two scoring mechanisms work as follows;
%\[
%    \gscore{t} = \frac{\rc{t}}{\ecg{t}}
%\]
%\[
%    \uscore{t} = \frac{\rc{t}}{\ecu{t}}
%\]
%where $\gscore{t}$ is the score generated from comparisons to the \textit{global} model and where $\uscore{t}$ is the score derived from the model representing the historical Tweets of $\aut{t}{O}$.


\section{Collecting the Training and Testing Data}
In order to train the model on a set of Tweets and then use it to make predictions, data was required for collection from Twitter. This data is relevant to the experiments and analyses in the following sections.

Since, in this case, it was necessary to collect the Tweet data along with each Tweet's numeric retweet count, rather than the binary nominal yes/no required in the previous chapter, only the retweets of a particular Tweet that had been created using the button method could be considered. This is because a Tweet's retweets executed using the manual copy and paste method do not contribute to the Tweet's official, and observable, retweet count that is returned from Twitter's API. This is not considered to be a limitation, however, since this factor is considered consistently through the training and later evaluation of the trained model.

In March 2013, a random walk was conducted through Twitter's social graph using v1.1 of Twitter's REST API. Although this date was before the mandatory transfer to this version of the API, the crawler method was used in preference to collecting from the public timeline, which was deprecated and removed in v1.1, so that user data could be collected to account for the importance of the social graph in information propagation.

Each stage of the walk consisted of focusing on and collecting information on a user, $u$. As such, the crawler is very similar to that used in the latter sections of the previous chapter. At each stage of the crawl, a set of recent Tweets, $T_u$, where $\aut{t}{O} = u \forall t \in T_u$ were collected. The size of $T_u$ had various dependencies, such as the $u$'s Tweet-posting frequency and the number of Tweets in total authored by $u$. Usually, several hundred Tweets from each user were yielded. In addition to the Tweet data, information on $u$ itself was collected as well as on a sample subset of up to 100, if they exist, of each of $\fos{u}$ and $\frs{u}$. The next stage involved focussing on a new user selected randomly from $\fos{u}$. Where `dead-ends' occurred (in cases where $\foc{u} = 0$ or the case where all $\fos{u}$ having already been collected), the crawler back-tracked to the most recently-collected user from which to select a valid follower to continue the crawl.

The sample subset of friends and followers of each user was collected instead of the complete set for the purposes of efficiency and to address the associated limitation in the previous interestingness inference methodology. However, the samples still provide an example snapshot of up to an additional 200 users in the author's neighbourhood in order to provide some idea of the activity within the local network both upstream and downstream from the author user. Around ten API calls were required to obtain this information for each user, giving it immediate advantages over the older method, which required several hundred or thousand depending on the particular user. 

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    symbolic x coords={{[0,$10^1$)},{[$10^1$,$10^2$)},{[$10^2$,$10^3$)},{[$10^3$,$10^4$)},{[$10^4$,$10^5$)},{[$10^5$,$10^6$)},{[$10^6$,$10^7$)},{[$10^7$,$10^8$)}},
        ylabel=Frequency,
        x tick label style={rotate=45, anchor=east},
        x label style={at={(axis description cs:0.5,-0.2)},anchor=north},
		xlabel=Count,
        ymin=0,
        ybar,
        bar width=7pt,
        width=15cm,
        height=7cm,
        legend entries={Follower count, Friend count}
        ]
   \addplot[plot 0,bar group size={0}{1}]
        coordinates { ({[0,$10^1$)},9) ({[$10^1$,$10^2$)},70) ({[$10^2$,$10^3$)},144) ({[$10^3$,$10^4$)},50) ({[$10^4$,$10^5$)},34) ({[$10^5$,$10^6$)},22) ({[$10^6$,$10^7$)},23) ({[$10^7$,$10^8$)},9)};
   \addplot[plot 1,bar group size={1}{1}]
        coordinates { ({[0,$10^1$)},7) ({[$10^1$,$10^2$)},40) ({[$10^2$,$10^3$)},196) ({[$10^3$,$10^4$)},96) ({[$10^4$,$10^5$)},24) ({[$10^5$,$10^6$)},7) ({[$10^6$,$10^7$)},0) ({[$10^7$,$10^8$)},0)};
\end{axis}
\end{tikzpicture}
\caption{Distribution of follower and friend counts of authors of Tweets in dataset $T$}
\label{fig:follower-friend-count_distribution}
\end{figure}

Importantly, Tweets from many different types of user were collected; from less-active users with very few followers and friends to influential users and celebrities with millions of followers and achieving many thousand retweets. The distribution of the follower and friend counts of the authors of Tweets in $T$ is displayed in Figure \ref{fig:follower-friend-count_distribution} and the collection of this wide range of users will help demonstrate if the new methodology is able to assess a larger variety of users and Tweets.

% dataset size, |T| = 240717
The data collection resulted in a set of Tweets, $T = T_u^1 \cup T_u^2 \cup ... \cup T_u^n$, where $|T| = 240,717$ and where $n = 370$. Of $T$, around 90,000 were Tweets with a retweet count of greater than zero. While the data was being collected, parts of the complete set were used in analyses of classifiers and Tweet categorisation methods to be used in the forthcoming experiments. $T_{p1} \subset T$, where $|T_{p1}| \approx 57,000$, represents the Tweets in $T$ part way through the data collection and is used in Section \ref{section:classification_performance}, and $T_{p2} \subset T$, where $|T_{p2}| \approx 67,000$, is used for the analyses in Section \ref{section:effects_of_varying_bin_sizes}. The entire set is used in the main validation analyses later on.


\section{Retweet Counts as Nominal Attributes}
Many machine learning classifiers are not able to accurately predict the outcome of a feature of a large-ranging and contiguous data type. Since retweet counts are on a very wide-ranging contiguous scale, using a classifier to make predictions from a limited range of discrete ranges, or `nominal' data, would be more appropriate.

Thus, in order to help improve the accuracy of $\ec{t}$ predictions, it was decided to convert the retweet count feature into a nominal data type for the purposes of training the model and making classifications. By `binning' the retweet counts into categories representing interval ranges, there would be fewer outcome possibilities, and thus the \textit{confidence} of classification could be greater.

The values for $\score{t}$ would then be determined through the ratio of $\rc{t}$ to the \textit{upper} limit of the category containing $\ec{t}$.

Generally, trained classifiers are only able to make predictions on features and values it has prior knowledge of. Therefore, the bin ranges for each category must be equal in both the training feature data and the testing feature data. If the available nominal values for an instance feature representing a Tweet has a different set of category ranges to that in the trained classifier model, then it is likely that a prediction cannot be generated for this instance. This is an important factor to consider when determining a method for binning the retweet counts.

One method to create bins based on a set of contiguous values is to do so in a \textit{linear} fashion. Given a set of retweet counts, $A$, where $A = \left\{\rc{t_1}, ..., \rc{t_n}\right\}$, then the range of retweet counts, $c_r = \max({A}) - \min({A})$. The linear approach then determines the range-size of each bin to be $\frac{c_r}{B}$, where $B$ is the desired number of bins. A particular retweet count, $\rc{t_i}$, can be assigned to a bin, $b$, if $x \leq \rc{t_i} < y$, where the interval describing $b = [x,y)$. The list of bins produced by the methodology represents the available nominal categories that each Tweet's retweet count can be assigned to.

In cases where $\min({A}) \neq 0$, the interval $[0,\min({A}))$ is pre-pended to the list of bins. Similarly, in all cases, the interval $[\max({A})+1,\infty)$ is appended to the list. This dictates that no Tweet in the set from which $A$ is derived can have a value for $\rc{t}$ categorised into this bin, and thus this allows any Tweet to potentially have $\score{t} > 1$ when testing against the eventual model. For example, if a training set of Tweets with a total range of values for $\rc{t}$ being between 1 and 20 was binned into four ranges, then the following interval categories would be applicable:

%  That is, that the full range of retweet counts in the training set was calculated and then split into bins such that each category had as equal a range as possible. If there were no cases where $\rc{t} = 0$, then a category representing $[0,l)$, where $l$ is the minimum value for the lowest range, was pre-pended to the set of available nominal categories. Similarly, in all cases, the interval $[m+1,\infty)$ was appended to the set of categories, where $m$ is the maximum value in the highest range. This dictates that no Tweet in the training set can have a value for $\rc{t}$ in this category, and thus this allows any Tweet to potentially have $\score{t} > 1$ when \textit{testing} against the model. For example, if a training set of Tweets had a total range of values for $\rc{t}$ being between 1 and 20 was binned into four ranges, then the following interval categories would be applicable:
\[
    [0,1), [1,6), [6,11), [11,16), [16,21), [21,\infty)
\]


\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{semilogyaxis}[
    symbolic x coords={{ },{[0, 8109)},{[8109, 16218)},{[16218, 24327)},{[24327, 32436)},{[32436, 40545)},{[40545, 48054)},{[48054, 56763)},{[56763, 64872)},{[64872, 72981)},{[72981, 81090)},{[81090, 89199)},{[89199, 97308)},{[97308, 810917)},{[810917, $\infty$)}},
        ylabel=No. Tweets assigned to bin,
        x tick label style={rotate=45, anchor=east},
        x label style={at={(axis description cs:0.5,-0.4)},anchor=north},
		xlabel=Bin intervals,
        xmin= ,
        ybar,
        bar width=7pt,
        width=15cm,
        height=7cm
        ]
   \addplot[plot 0,bar group size={0}{1}]
        coordinates {({ },0) ({[0, 8109)},236985) ({[8109, 16218)},1367) ({[16218, 24327)},718) ({[24327, 32436)},530) ({[32436, 40545)},347) ({[40545, 48054)},229) ({[48054, 56763)},165) ({[56763, 64872)},106) ({[64872, 72981)},69) ({[72981, 81090)},67) ({[81090, 89199)},33) ({[89199, 97308)},28) ({[97308, 810917)},73) ({[810917, $\infty$)},0)};
        
\end{semilogyaxis}
\end{tikzpicture}
\caption{Bin intervals and cardinalities for the retweet counts of Tweets in $T$ linearly binned with $B=30$}
\label{fig:bin_distribution_linear}
\end{figure}

Since the distribution of retweet counts (expressed through retweet group sizes) is known \cite{webberley11}, then it is clear that this binning methodology would produce bins containing a very non-uniform distribution of Tweets, where the lower bin ranges would contain many Tweets and the cardinality of each category would decrease exponentially as the categories become higher. Figure \ref{fig:bin_distribution_linear} illustrates the non-uniformity of the resultant bin sizes for the set of Tweets, $T$, linearly binned even with an exaggerated $B = 30$. The latter intervals are combined in the Figure for readability. The result of this means that there are significantly fewer feature instances representing Tweets with larger retweet counts.

Indeed, when training a Bayesian Network classifier and using it to run cross-validations on the retweet counts of Tweets in $T$, this binning scheme demonstrated a high accuracy of predictions on Tweets with lower values for $\rc{t}$ and a low accuracy for Tweets with higher counts. Table \ref{table:linear_bin_performance} shows this high precision and recall for the bin containing the majority of the Tweets, along with the very poor performance of the subsequent bins. Only the first ten bins are shown, since the remaining 20 bins produced precision and recall values of 0. It would be more appropriate, and better address the desire for more universal use-cases expressed earlier in this and the previous chapter, if the accuracy of predictions could be more uniform across the bin ranges.

% Data from linear-distributed_comparison.txt
\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ l | l | l }
	Retweet count bin interval	& Precision & Recall \\
	\hline
	\hline 
    {[0, 8109)}        &   1.0     &   0.956\\
    {[8109, 16218)}    &   0.083   &   0.355\\
    {[16218, 24327)}  &   0.134   &   0.315\\
    {[24327, 32436)}  &   0.233   &   0.072\\
    {[32436, 40545)}  &   0.0       &   0.0\\ 
    {[40545, 48654)}  &   0.008   &   0.004\\
    {[48654, 56763)}  &   0.105   &   0.109\\
    {[56763, 64872)}  &   0.03    &   0.038\\
    {[64872, 72981)}  &   0.008   &   0.174\\
    {[72981, 81090)}  &   0.009   &   0.343\\
    \hline  
\end{tabular}
\end{center}
\caption{Cross-validation performance results for the first 10 bins produced through the linear binning method on retweet counts in $T$ with $B=30$}
\label{table:linear_bin_performance}
\end{table}

A responsive approach dependent on the range and distribution of retweet counts would help in producing more evenly-filled bins and therefore increase the prediction performance across the range of intervals. A new methodology is proposed, following a retweet count distribution based approach, which uses the size of the Tweet set to be categorised and the bin count $B$ for dynamically generating interval ranges such that the cardinalities of each bin are as even as possible. Each bin can then be filled according to the bounds of its interval, and in such a way as to ensure that each retweet count frequency would only be present in one bin. For example, all of the retweets achieving one retweet would be placed in the single bin encompassing this value. Algorithm \ref{algo2} illustrates this dynamic approach in more detail.

As such, after the intervals covering the bin bounds have been produced, then these represent the nominal categories for the retweet count feature in each instance for training and testing against the classifier.

\newfloat{algorithm}{H}{lop}
\begin{algorithm}
\caption{Algorithm for producing intervals for bin categories for $\rc{t}$ values.}
\begin{algorithmic}[1]
\Procedure{generate\_intervals}{set of Tweets $T$, number of bins $B$}
    \State $C\gets$ empty list \Comment{To hold ordered retweet counts}
    \State $I\gets$ empty list \Comment{To represent bin range intervals}
    \ForAll{$t \in T$}
        \State Add $\rc{t}$ to $C$
    \EndFor
    \State Sort $C$ into ascending order 
    \State $M\gets\max(C)$ \Comment{Highest instance of $\rc{t}$}
    \State $T\textrm{Sum}\gets\lceil\frac{|C|}{B}\rceil$ \Comment{Number of Tweets to be held by each bin}
    \State $H\gets$ empty dictionary \Comment{To represent the distribution of retweet counts}
    \Statex
    \ForAll{$c \in C$}
        \If{$c \in H$}
            \State Increment $H_c$
        \Else
            \State $H_c\gets1$
        \EndIf
    \EndFor
    \ForAll{$i$ in range $1, ..., M+1$}
        \If{$i \in H$}
            \State $s\gets s + H_i$
        \EndIf
        \If{$s \geq T\textrm{Sum}$}
            \State Add $i$ to $I$
        \EndIf
    \EndFor
    \State Return $I$
\EndProcedure
\end{algorithmic}
\label{algo2}
\end{algorithm}
 
This method readily supports more uniform bin sizes, and copes with this by exhibiting exponentially larger bin \textit{ranges}. As with the linear method, the interval $[0,\min({A}))$ is pre-pended, where the dataset requires it, and $[\max({A})+1,\infty)$ is always appended in addition to the intervals produced by the algorithm. The distribution of bins for the retweet counts for the Tweets in $T$ when binned through this method is illustrated by Figure \ref{fig:bin_distribution_distributed}. The same logarithmic scale is used as in Figure \ref{fig:bin_distribution_distributed} to allow the comparison to help illustrate the greater uniformity in this dynamic method.

%\begin{figure}[h]
%\centering
%\begin{tikzpicture}
%\begin{semilogyaxis}[
%    symbolic x coords={[0-1), [1-2), [2-3), [3-4), [4-5), [5-100)},
%        ylabel=Cardinality of bin,
%		xlabel=Bins,
%        ybar,
%        bar width=7pt,
%        yticklabels={,,},
%        xticklabels={,,}
%        ]
%   \addplot[plot 0,bar group size={0}{1}]
%        coordinates {([0-1),100) ([1-2),50)  ([2-3),50) ([3-4), 50) ([4-5), 50) ([5-100), 25)};
%\end{semilogyaxis}
%\end{tikzpicture}
%\caption{Example distribution of retweet count bin cardinalities under the responsive binning algorithm}
%\label{fig:bin-hist}
%\end{figure}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{semilogyaxis}[
    symbolic x coords={{[0,1)},{[1,3)},{[3,8)},{[8,16)},{[16,29)},{[29,58)},{[58,147)},{[147,512)},{[512,3301)},{[3301,810917)},{[810917,$\infty$)}},
        ylabel=No. Tweets assigned to bin,
        x tick label style={rotate=45, anchor=east},
        x label style={at={(axis description cs:0.5,-0.4)},anchor=north},
		xlabel=Bin intervals,
        ymin=10e0,
        ybar,
        bar width=7pt,
        width=15cm,
        height=7cm
        ]
   \addplot[plot 0,bar group size={0}{1}]
        coordinates {({[0,1)},150609) ({[1,3)},21834) ({[3,8)},11462) ({[8,16)},8926) ({[16,29)},8487) ({[29,58)},8499) ({[58,147)},8136) ({[147,512)},8070) ({[512,3301)},8039) ({[3301,810917)},6655) ({[810917,$\infty$)},0)};
   \draw [red] ({rel axis cs:0,0}|-{axis cs:{[0,1)},8023.9}) -- ({rel axis cs:1,0}|-{axis cs:{[810917,$\infty$)},8023.9}) node [pos=0.95, above] {$\lceil\frac{|C|}{B}\rceil$}; 
\end{semilogyaxis}
\end{tikzpicture}
\caption{Bin intervals and cardinalities for the retweet counts of Tweets in $T$ dynamically binned with $B=15$}
\label{fig:bin_distribution_distributed}
\end{figure}

The responsiveness stems from the fact that the bin ranges adapt to the variety and number of retweet counts present, and the method always attempts to produce a similar number of bins to the target count. However, due to the disproportionately large number of small retweet groups, the bin sizes cannot be entirely uniform and this means that the number of bins returned will generally be smaller than the target number. Furthermore, a single Tweet cannot exist in more than one bin concurrently. In the case of Figure \ref{fig:bin_distribution_distributed}, the number of Tweets with retweet count in the interval $[0,1)$ is greater than $\lceil\frac{|C|}{B}\rceil$, where $C$ is the ordered list of retweet counts of Tweets in $T$, resulting in a significantly larger first bin and an overall bin count of less than $B$. Without this particular feature, training Tweets with equal observed retweet count may be categorised into multiple bins, which may cause complications with the training and eventual testing of the model. 

The performance given by cross validations of a Bayesian Network classifier on the retweet counts of Tweets in $T$ is given by Table \ref{table:distributed_bin_performance}. It is clear that the precision and recall accuracy is much more evenly balanced across the bin intervals generated by the dynamic method than those by the linear method, the latter of which were reported in Table \ref{table:linear_bin_performance}.

% Data from linear-distributed_comparison.txt
\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ l | l | l }
	Retweet count bin interval	& Precision & Recall \\
	\hline
	\hline 
    {[0, 1)}           &    0.935   &   0.741\\
    {[1,3)}             &   0.218   &   0.324\\
    {[3,8)}             &   0.190    &   0.394\\
    {[8,16)}            &   0.240   &   0.233\\
    {[16,29)}           &   0.291   &   0.298\\
    {[29,58)}           &   0.265   &   0.338\\
    {[58,147)}          &   0.232   &   0.201\\
    {[147,512)}         &   0.256   &   0.418\\
    {[512,3301)}        &   0.527   &   0.508\\
    {[3301,810917)}     &   0.519   &   0.709\\
    \hline  
\end{tabular}
\end{center}
\caption{Cross-validation performance results for the first 10 bins produced through the dynamic binning method on retweet counts in $T$ with $B=15$}
\label{table:distributed_bin_performance}
\end{table}

Due to this dynamicity, the bin ranges and cardinalities produced by the algorithm vary across different datasets. As a result, the nominal bin categories generated for producing the value for $\ecg{t}$ from the user model trained from the complete set of collected Tweets posted by $\aut{t}{O}$ would be different from those categories generated for a different user. The intervals in each bin category are therefore reflective of the various numbers of retweets that each author's Tweets are likely to receive. 

Thus, when testing against either the global or user models, the Tweets' retweet counts are predicted as \textit{nominal} counts equal to the upper bound of the bin category the Tweets are classified as. For example, consider a Tweet, $t$, to be tested against a classifier that was trained by a training set of Tweets that produced the bin intervals shown in Table \ref{table:distributed_bin_performance}. This Tweet has $\rc{t} = 20$, but is, in this example, classified as $\ec{t} =  {[8,16)}$, and therefore receives a score of $\score{t} = \frac{20}{16} = 1.25$.

Similarly, when training, the bin intervals are first generated from the training set's retweet counts. Each Tweet then has its retweet count transformed into the generated nominal bin interval that surrounds the Tweet's retweet count before the model is trained. If the same training set is used as in the previous example and one of the Tweets in the set has a retweet count of 48, then this Tweet has its count transformed into the nominal {[29,58)} for training with.
 

\section{Predicting Estimated Retweet Counts}
Since the environment has previously been found to have a large effect on propagation, then the features describing a Tweet's author's relationships and local network activity  are a useful aid in feature selection, as described by references to memetics in the Background chapter. 
 
In order to generate the estimated retweet counts, a trained machine learning classifier is used to make predictions on a set of instances made up from both environmental and Tweet features. This section covers the process of selecting a classifier and justifying its use in terms of analysing its performance.


\subsection{The Classifier}
An overview of machine learning classifiers and their processes was provided in the previous chapter. In that case, a logistic regression was used to generate a prediction on a binary retweet decision based on a small number of features. If the retweet count for the Tweet being trained or tested was greater than zero, then the retweet decision would be positive (\textsc{True}). Otherwise, the decision was negative (\textsc{False}).

The improved methodology proposed in this chapter involves the prediction of a retweet count category from a set of nominal values of cardinality greater than one. As mentioned, the instances of a particular Tweet and its environment are categorised based on the value of the retweet count of the Tweet. Although this means that a degree of accuracy is sacrificed when training the classifier, it does mean that there are fewer categories for predictions on test Tweet feature instances, providing a higher confidence for each prediction made.

A Bayesian network machine learning classifier was elected for use for the purposes required in this chapter. Use of this classifier in the social media domain is more rare than other classifiers, such as those involving a regression or a decision tree, but was selected due to its performance and efficiency shown in Table \ref{table:classifierperformance}.  

The Bayesian network is an unsupervised classifier since its learning algorithms do not simply determine the class of the outcome, the retweet count, from the attribute features alone \cite{friedman97}. Instead, a probabilistic graph is constructed based on the dependencies between the variables. The variable attributes form the nodes of the graph and edges between the nodes denote the dependencies (or lack thereof) between them.

In the case of this research, the various Tweet and environmental features, including the nominal retweet count, form the nodes in the Bayesian Network. When forming the network through training, the dependencies and their probabilistic weightings are adjusted so that an expected value for the retweet count can then be `predicted' from the values of all the other variable attributes.


\subsection{Classification Performance}
\label{section:classification_performance}
The choice of classifier stems from its combined efficiency and accuracy when working on the data relevant to this chapter. This section provides a brief overview and comparison of a subset of commonly-used (and related) classifiers in social network research.


\subsubsection{Choosing a classifier}
In order to study and evaluate the relative performance of appropriate classifiers for selection for this task, the Weka\footnote{http://www.cs.waikato.ac.nz/ml/weka} machine learning and data mining toolkit was used. The classifiers were selected to cover a sample of the range of available classifier categories. Whilst some types may work inefficiently in this scenario, it is likely that they are more efficient when employed in different use-cases.

Although the accuracy of prediction was important, it would also be useful for the classifier to be \textit{efficient} in training its model and when testing future instances against it. This is so that this method could be used to produce interestingness inferences with a more on-demand policy and to further improve on the methodologies used in the previous chapter.

\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ l | c | c | c }
	Classifier	& Weighted average precision & Weighted average recall &  Training time (secs.) \\
	\hline
	\hline 
	Simple logistic & 52\% &  56\% & 528\\
    Logistic        & 62\% &  56\% & 18\\
    SMO             & 51\% &  55\% & 1384\\
    Na\"{i}ve Bayesian & 50\% & 44\% & 0.13\\
    Bayesian network & 62\%&  64\% & 0.54\\
    \hline  
\end{tabular}
\end{center}
\caption{The performance of different machine learning classifiers in cross-validations on dataset $T_{p1}$}
\label{table:classifierperformance}
\end{table}

Table \ref{table:classifierperformance} shows the Bayesian network to be accurate and time efficient when evaluating the performance of the set of evaluated classifiers when running cross-validations on the dataset $T_{p1}$, which is a subset of $T$ and contains around 57,000 Tweets.  For each Tweet instance the same retweet count binning scheme was used, and each classifier performed the same number of cross-validations against the same dataset in order to obtain the precision and recall values.

Although the dataset used in this analysis is not the complete set used in practice, the cardinality of the dataset was sufficient to cause the outputs to be indicative of the Bayesian network's relative advantages over the other assessed classifiers.


\subsubsection{Overview \& evaluation of classifier performance}
The data observed in Table \ref{table:classifierperformance} illustrate how different algorithms approach classification in various ways. In addition to the \textit{type} of the data in the instance features, the cardinality of the dataset can also impact training efficiency. Following is an overview of the different classifiers assessed. `Independent variables' represent the input values to be classified in each case, and a `dependent variable' is the output class given by the inputs. 

% independent = inputs, dependent = outputs
\textbf{Simple logistic}\\
Linear regression involves the prediction of a dependent scalar value dervied from a set of independent variables. For example, consider the case of one dependent, one independent variable, and their modelled relationship. Previously-seen values for the independent variable can be used to estimate an associated value for the dependent variable. Predictions for the dependent variable with unseen independent instances can be inferred from surrounding `known' instances.

Simple logistic regression is analogous to linear regression, except the dependent variable is nominal, and the regression allows for producing probabilities that the independent variables belong to a particular class \cite{sumner05}.

\textbf{Logistic}\\
As with its `simple' counterpart, logistic regression involves the prediction of a nominal variable derived from a set of independent values. Analyses in this scope are categorised as binary (two nominal classes) or multinomial (three or more binary output classes), and the output probability is produced through the natural logarithm of the dependent variable being the case given the independent variables.

\textbf{SMO}\\
Sequential minimal optimisation (SMO) is an algorithm for producing support vector machines (SVMs) quickly and cheaply \cite{platt98}. SVMs are non-probabilistic graph models for nominal classification, and work by representing independent variables in space, but separated into clusters representing the class of the dependent variable. The output class of new independent variables is given by the cluster they are mapped to.

\textbf{Na\"{i}ve Bayesian}\\
Na\"{i}ve Bayesian classification is able to consider many multi-dimensional (including nominal and scalar) features. In particular, Bayesian probability is applied towards prediction of the value of the dependent variable, yet \textit{without} considering any relationships or weightings between the independent variables. For example, a soda bottle is long, thin, and cylindrical. A na\"{i}ve Bayesian classifier trained on these independent variables may still classify instances as being a soda bottle if there is enough evidence to suggest it is cylindrical, even if it is \textit{not} long and thin.

\textbf{Bayesian network}\\
A Bayesian network classifier involves a graph model, whose nodes represent the independent and dependent variables and whose edges indicate the influences between variables. Upon training and testing, a Bayesian probability distribution is generated over the variables, which is used to indicate the resultant class of the dependent variable given the relationships between the independent variables and itself.

\textbf{Evaluation}\\
As seen in Table \ref{table:classifierperformance}, the two Bayesian probability-based approaches are significantly faster than the logistic and support vector machine methods. Classification through logistic regression involves further searches through its matrix model upon addition of training instances\footnote{http://www.academia.edu/5167325/Weka\_Classifiers\_Summary}, which takes longer to complete as training data siza increases. Similarly, when producing the support vector machines, the sequential minimal optimisation algorithm must normalise the input features as new instances are added.

 The Bayesian network is able to model relationships between indepdendent variables, and the na\"{i}ve Bayesian classifier is not. This means that these relationships allow for representation of the ties between features in the underlying social graph, such as observed relationships between follower and friend count for `measuring' influence. Therefore, they can be used to more accurately infer a projected retweet count than when they are not considered, and the more complex computation accounts for the quicker training time achievable with the na\"{i}ve approach. 


\subsection{Effects of varying the Cardinality of Nominal Retweet Counts}
\label{section:effects_of_varying_bin_sizes}
Applying the continuous retweet count values to produce a set of nominal categories representing interval ranges of the retweet counts requires a certain balance. By reducing the number of target category bins then the classification accuracy increases, but the level of applicability of the eventual interestingness score for the wide range of retweet counts observed would be reduced since the \textit{granularity} of the predicted counts would also decrease. Conversely, with more bins, the classification accuracy reduces, as there would be fewer instances in each category, yet the scores would be apliccable to a wider range of retweet counts.

\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ c | c | c | c }
	Target bin count ($B$)	& Resultant bin count & Weighted average precision &  Weighted average recall \\
	\hline
	\hline 
    1 & 1 & 100\% & 100\% \\
    2 & 2 & 89.3\% & 89.3\% \\
    5 & 4 & 78.8\%  & 74.5\% \\
    10 & 7 & 68.6\% & 65.7\% \\
    15 & 10 & 61.2\% & 56.4\% \\
    20 & 12 & 59.1\% & 52.9\% \\
    25 & 15 & 51.4\% & 47.5\% \\
    30 & 18 & 49.3\% & 45.3\% \\
    35 & 21 & 47.2\% & 43.2\% \\
    40 & 23 & 46.2\% & 42.5\% \\
    \hline  
\end{tabular}
\end{center}
\caption{The effect of varying $B$ on the cross-validation performance using a Bayesian network classifier on dataset $T_{p2}$}
\label{table:binperformance}
\end{table}

Clearly, by increasing the number of nominal categories used, then the relative number of feature instances in each eventual interval decreases. These bins represent the nominal categories that each feature instance is classified as in relation to the predicted retweet count of the instance. Table \ref{table:binperformance} outlines the decrease in classification accuracy observed with increases in target bin count using the dataset $T_{p2}$.

In the upcoming experiments a value of $B = 14$ was used, which yielded ten nominal retweet count categories for use in training and testing against the general global dataset for the purposes of generating the global expected retweet count. Since each user's own retweet count ranges were different, the number of categories were calculated individually for producing the user-centric expected retweet counts as part of calculating values for $\uscore{t}$.


\section{Training and Testing Against the Classifier}
This section discusses the processes used to calculate interestingness scores for Tweets through the generation of expected retweet counts using the methodologies outlined in the previous sections. Particular focus is lended to the managing of the data corpora and feature extraction.


\subsection{Data Corpora}
The dataset $T$ was collected, as described earlier, and is used for training the model and for validity testing. As such, $T$  was divided into two datasets; a training set, denoted $\trainset{}$ and consisting of 90\% of the entire set, and a testing dataset, denoted by $\testset{}$ and consisting of the remaining 10\%. $T$ was divided in such a way as to ensure that all of the Tweets authored by one particular user existed in only one of the two resultant datasets. After being used to train the Bayesian network model, $\trainset{}$ was then discarded from use for the rest of the experimentation.

In order to support the user scores, in addition to the global scores, further datasets are required to be extracted from $\testset{}$ to produce an individual $\userset{u}$ for each $t \in \testset{}$ where $\aut{t}{O} = u$. Each of these subsets contains the Tweets written by and information about that particular user, and can be used to train an individual Bayesian network classifier specific to the user. $\testset{}$ is therefore referred to as the `global' Tweet corpus for testing against the global model to generate global Tweet scores, and $\userset{u}$ is referred to as the `user' Tweet corpus for user $u$ in order to train that author's user model and for testing the Tweets it posts to generate \textit{user} Tweet scores. Each Tweet, $t$, therefore, can be evaluated against both the global model and its author's user model in order to respectively produce values for $\ecg{t}$ and $\ecu{t}$.

It should be noted that the same user corpus is used for both training the model and for testing against in the validations of the user scores. This is due to the relatively low number of Tweets available for training each individual user model, and so re-use of the Tweets is necessary for the research in this chapter. The process for generating the global scores still maintains distinct datasets for training ($\trainset{}$) and testing ($\testset{}$).

%As has been previously mentioned, of interest is the generation of \textit{two} interestingness scores for each Tweet, $t$; one based on a comparison between $\rc{t}$ and an $\ec{t}$ produced from the global model, and the other between $\rc{t}$ and an $\ec{t}$ produced from $\aut{t}{O}$'s user model. As such, each Tweet requires the two models in order to provide the predicted values for $\ec{t}$. To assist with this, individual user datasets were extracted from the testing dataset, each containing information only on that particular user and its local network and its Tweets, from which individual user Bayesian network models could be trained.

%The complete testing dataset is referred to as the `global' corpus of Tweets, and each individual user dataset is known as a `user' corpus.



\subsection{Features}
Producing the instances used for testing and training the Bayesian network models involved the extraction of various features from the global and user datasets. Generally, each feature falls into one of three categories; the network features (`environment'), the Tweet features (`genome'), and the author features (representing the author of the current Tweet). The nominalised retweet count is categorised as a Tweet feature.

Generally, the Tweet features follow the same notions as those used in the previous chapter in that they are static and generally binary features describing various aspects of the Tweet's content and metadata. The network features are more variable and describe the ways in which the author's local network is constructed and the activity within it.

Each Tweet is represented by an instance of a complete set of features relating to that Tweet, its author, and its author's local network. As a result, feature instances representing Tweets authored by the same user will share the same values for their network and author features.


\subsubsection{Features for the global corpus model}
The global corpus model is the Bayesian network model representing the classifier trained from the complete training dataset. In this case, a total of 31 features, outlined in Table \ref{table:globalfeatures}, were used to train the classifier. As such, there were around 217,000 Tweet instances using this feature scheme used for training the global classifier.

\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ c | l | c }
	 Feature category	& Feature & Feature data type \\
	 \hline
	 \hline 
	& mention & \{True, False\}\\
    & Tweet length & real (numeric)\\
    & url & \{True, False\}\\
  	& hashtag & \{True, False\}\\
  	Tweet & positive emoticon & \{True, False\}\\
  	(`genome')& negative emoticon & \{True, False\}\\
  	& exclamation mark & \{True, False\}\\
  	& question mark & \{True, False\}\\
  	& starts with `RT' & \{True, False\}\\
  	& is an @-reply & \{True, False\}\\
    & \textbf{retweet count} & \textbf{[dynamic nominal]}\\
  \hline                        
	& follower count & real (numeric)\\
    & friend count  & real (numeric)\\
	Author & verified account & \{True, False\}\\
	& status count & real (numeric)\\
	& listed count & real (numeric)\\
    \hline
  	&  max. follower count & real (numeric)\\
	&  min. follower count & real (numeric)\\
	& avg. follower count & real (numeric)\\
    Network & max. friend count & real (numeric)\\
	(`environment') & min. friend count & real (numeric)\\
	& avg. friend count & real (numeric)\\  
	& avg. status count & real (numeric)\\  
  	& proportion verified & real (numeric)\\  
  \hline  
\end{tabular}
\end{center}
\caption{Features used to train the model from the global data corpus.}
\label{table:globalfeatures}
\end{table}

The network features listed apply to both samples of the followers and friends retrieved for each author user during the data collection. For example, the first feature of this category, `max. follower count', represents two features referring to the maximum follower count observed across the sample of the user's followers and the sample of the user's friends respectively.

It should be noted that although the Tweet features, aside from the retweet count as has already been discussed, are permanent after the Tweet has been created and posted, the author and network features are more dynamic due to the continuous mutations in the social graph as edges representing followships are constantly being formed and broken between the user nodes. In this thesis, it is assumed that changes to the features representing these factors were not significant over the period of posted Tweets for each user, and the effect is minimised through consideration only of the recent Tweets of each author user.

After training the classifier with these features from the set $\trainset{}$, each Tweet, $t \in \testset{}$ was tested against the model in order to classify it into a retweet outcome category, as described above. The upper bound of this category interval is then used, along with $\rc{t}$ to assign $t$ a numeric global score, $\gscore{t}$.


\subsubsection{Features for individual user models}
Since the author and network features have identical values in the instances representing all of the Tweets from one particular user, then these features were not considered when training and testing using the user models. As such, the 10 Tweet features were those used in the feature instances in training, and testing against, each user model.

After training each user classifier with the features representing that particular user and its Tweets, each Tweet $t \in \testset{}$ was tested against the classifier model representing the features of $\aut{t}{O}$ in order to assign it a numeric user score, $\uscore{t}$.  

%\subsection{Testing Against the Trained Classifier}
%Once the feature extraction was completed and the instances were built for the global training dataset and each individual user set, the models were trained as described above.
%
%In order to produce the expected \textit{global} retweet counts, each Tweet $t \in $\testset{} had its features extracted and was evaluated against the global model trained from \trainset{}. This classified each Tweet into one of the categories given by that Tweet's predicted retweet count, and the top interval of the expected retweet outcome category became the \textit{expected} retweet count for that particular Tweet. Similarly, the expected \textit{user} retweet counts were produced in the same way, but instead each Tweet was classified by the user model associated with that Tweet's author.
%
%In each case, the two interestingness scores for each Tweet could be calculated, based on the process described earlier, using the ratio between the Tweet's two expected retweet counts and its \textit{observed} retweet count stored as part of the data collection from Twitter. This meant that each Tweet $t \in T$ then had two numeric scores, $\uscore{t}$ and $\gscore{t}$, assigned to it. 



\section{Initial Validations of the Scoring Methodologies}
In order to verify the accuracy of the assignment of both of the scores to each Tweet in $\testset{}$, validation tests by human participants was required. Through running these validations, the relative performance of the scoring mechanism can be assessed, and the comparative performance of the two scores, $\uscore{t}$ and $\gscore{t}$ can be evaluated.

Mechanical Turk was again used to crowdsource inputs from MTWs, as this would facilitate the obtaining of interestingness evaluations from a wider range of human opinion.


\subsection{Planning the Validations}
The MTWs taking part would not be associated with the collected Tweets in any way, and thus this assists in the identification of the non-noisy Tweets that are `globally' interesting and are those that the scores have theoretically determined as `interesting'.

At this stage, certain Tweets and users were removed from $\testset{}$. Since the Tweet and user data was collected as part of a random crawl, there was no governance over the content of the Tweets collected. As such, users who frequently used offensive phrases or did not write Tweets in English had their Tweets removed from $\testset{}$ for the validations, since the Mechanical Turk microtasks were submitted to be assessed by MTWs from the USA. As before, individual Tweets that were `@-replies' were also removed so that only Tweets intended for broadcast were included in the final set to be evaluated.

%Certain Tweets and users were removed, at this stage, from the dataset of Tweets to be assessed by the MTWs. Since the Tweet data was collected through a random crawl through Twitter and no checks were placed on the crawler at that stage, there was no governance over the content of the text in each Tweet of the data. Therefore, users who frequently used offensive phrases or wrote Tweets in non-English had their Tweets removed. The reasoning behind the latter point is based on the fact that the Mechanical Turk microtasks were submitted to be completed by people living in the USA. As before, individual Tweets that were `@-replies' were also stripped so that only Tweets intended to be broadcasts were included in the final MTW test set.


\subsection{Carrying Out the Validations}
In the context of this validation scheme, MTWs were not required to hold an active Twitter account. By not determining the humans to make the assessments, a more diverse opinion on the interestingness can be achieved, as the different users will have varying considerations on what constitutes `noise' and will therefore reinforce a decision when multiple MTWs form agreements on what is interesting. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{5.Chapter3/Media/questions1.png} 
\caption{An example validation question.}
\label{fig:example_question1}
\end{figure}

The validations were carried out such that the MTWs were presented with a series of questions, each of which consisting of five different Tweets from \textit{one} specific author. An example question is shown in Figure \ref{fig:example_question1} and, as such, Tweets were assessed against others that had been posted by the same user. In each question, the MTWs were asked to select the Tweets that they consider to be the most interesting of the group, and that they must select at least one Tweet for each question. For each judgment, where a judgment is one question answered with one or more Tweets selected, MTWs were paid \$0.05. Given the relative shortness of each Tweet, each judgment was expected to take less than 30 seconds to complete (including quickly visiting URLs in those Tweets that contained them), meaning that MTWs would be paid an average of at least \$6.00 per hour if completed continuously. This rate is generally acceptable for workers completing this type of easier, multiple-choice task and for newer workers\footnote{http://blog.echen.me/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices}.

The test was conducted under the conditions of a randomised controlled trial. To this end, each Tweet was assessed in three different contexts, in that it would appear in three different questions alongside four other randomly chosen Tweets, and that each question would then be judged by three different MTWs.

From the stripped testing dataset, 750 Tweets were selected, at random, to be divided by user into the questions to be assessed on Mechanical Turk. Since each Tweet was to appear in three different questions and since each question consisted of five unique Tweets, then this resulted in a total of 450 distinct questions. Each Tweet was assessed as part of a question nine times in total. 


\subsection{Outcomes From the Validations}
The validation test involved contributions from 91 different MTWs in total, and 325 of the 450 questions in total asked had responses where a Tweet was selected \textit{confidently}. A confidently-answered question is defined as the case when at least two of the three respondent MTWs answering that question selected the same Tweet. Since the MTWs had the opportunity to select more than one Tweet of each question to be the most interesting, there were 349 Tweets of the original 750 Tweets, denoted as $T' : T' \subset \testset{}$, that were selected as sufficiently interesting by the MTWs. Tweets selected from individual questions that did not have sufficient confidence were discarded.

The remainder of this section analyses the validation data in various ways to demonstrate the strengths and weaknesses of the interestingness score inferences. Of immediate notice was the comparative difference between the two different scoring mechanisms for each Tweet $t$; $\gscore{t}$ and $\uscore{t}$. The inference validation results are not significant between the use of the two scores in any of the analyses conducted. 

\subsubsection{General Performance}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{semilogyaxis}[
    symbolic x coords={{[0,1)}, {[1,2)}, {[2,3)},{[3,4)}, {[4,5)}, {[5,100)}}, % inside braces to support commas in intervals
        ylabel=Proportionate frequency,
		xlabel=$\gscore{t}$,
        ymin=1,
        legend pos=north east,
        legend style={nodes=right},
        ybar,
        bar width=7pt,
        legend entries={$T'$,  $\testset{}$}
        ]
   \addplot[plot 0,bar group size={0}{2}]
        coordinates {({[0,1)},76.30057803) ({[1,2)},7.514450867)  ({[2,3)},4.335260116) ({[3,4)}, 1.445086705) ({[4,5)}, 2.023121387) ({[5,100)}, 6.936416185)};
        \addplot[plot 1,bar group size={1}{2}]
        coordinates {({[0,1)},80.94365552) ({[1,2)},6.596426935)  ({[2,3)},3.710490151) ({[3,4)}, 1.099404489) ({[4,5)}, 0.961978928) ({[5,100)}, 4.634448007)};
        
\end{semilogyaxis}
\end{tikzpicture}
\caption{Proportionate frequency distribution of $\gscore{t} \forall t \in \testset{}$ compared to only those $\gscore{t} \forall t \in T'$}
\label{fig:hist}
\end{figure}

Of the subset $T'$, the scoring mechanism found 140 of the Tweets to have a value of $\gscore{t} > 1$, and thus inferred as \textit{more} interesting than the remaining Tweets. Of these, 65\% were agreed on as interesting by the MTWs. The performance of the $\uscore{t}$ was worse in providing a 55\% agreement, resulting in a general of 60\% agreement on the mean of the two scoring schemes.

\begin{myobservation}
    Although not significant, the general performance accuracy is demonstrably greater when using the \textbf{global} scoring scheme than the \textbf{user} scores.
\end{myobservation}

It is also the case that the proportionate frequency of Tweets with higher values of $\gscore{t}$ is greater in the subset $T'$ than in $T$. This implies that, on average, Tweets selected by MTWs have a higher score than those not selected, as is shown in Figure \ref{fig:hist}. Additionally, a greater proportion of Tweets with $\gscore{t} < 1$ were in $\testset{}$ than in $T'$ and a greater proportion of Tweets with $\gscore{t} > 1$ were in $T'$ than in $\testset{}$.

\begin{myobservation}
    MTWs select, on average, more Tweets with an `interesting score' ($\gscore{t} > 1$) than Tweets with a `non-interesting score' ($\gscore{t} < 1$).
\end{myobservation}


\subsubsection{Ranking Performance}

In order to assess the ability of the scores to effectively rank Tweets in order of inferred interest \textit{level}, the Tweets were studied on a per-question basis.

\begin{figure}[h]
\centering
\begin{tikzpicture}
 \begin{axis}[
        xlabel=$n$,
        ylabel=Likelihood of selection (\%),
        grid = major,
        legend entries={where $\gscore{t^q_1} \geq 0$, where $\gscore{t^q_1} \geq 1$, where $\gscore{t^q_1} \geq 2$, where $\gscore{t^q_1} \geq 6$, Random selection},
		legend style={at={(1.7,0.5)}},
		xmin=0, xmax=5,
		ymin=0, ymax=100
		]
	\addplot[mark=*,black] plot coordinates {
        (0,0) (1,50.8) (2,85.3) (3,94.3) (4,98) (5,100)
        };
        
	\addplot[mark=*,cyan] plot coordinates {
        (0,0) (1,32.5) (2,65.2) (3,86.95) (4,95.4) (5,100)
        };
    \addplot[mark=*,orange] plot coordinates {
       (0,0) (1,30.4) (2,56.3) (3,81.1) (4,94.15) (5,100)
    };
    \addplot[mark=*,red] plot coordinates {
        (0,0) (1,30.9) (2,53.2) (3,75.7) (4,88.6) (5,100)
    };
    \addplot[gray] plot coordinates {
        (0,0) (1,20) (2,40) (3,60) (4,80) (5,100)
    };
\end{axis}
\end{tikzpicture}
\caption{Likelihood of MTWs selecting one of the top $n$ Tweets ranked by $\gscore{t}$ in each $q \in Q$. Also illustrating the effect of raising the minimum allowed $\gscore{t^q_1}$}
\label{fig:score-dist}
\end{figure}

Let $q$, which represents a particular question containing five Tweets, $t^q_i \quad \forall \quad 1 \leq i \leq 5$, be ranked in order of ascending interestingness score such that;
\[
    q = (t^q_1, t^q_2, t^q_3, t^q_4, t^q_5)
\]
where; 
\[
    \gscore{t^q_1} \geq \gscore{t^q_2} \geq ... \geq \gscore{t^q_5}
\]

Let $Q$ be the set of 450 such ordered questions asked of the MTWs, such that;
\[
    Q = \left\{q_1, q_2, ..., q_{450}\right\}
\]

In cases where $\sum\limits_{j=1}^5 \gscore{t^{q_i}_j} = 0$, $q_i$ is removed from $Q$ as none of $t^{q_i}_1,..., t^{q_i}_5$ have been ranked as more interesting than one another or as interesting at all. 

Given that the Tweets in each question, $q \in Q$, have been ranked in order of inferred interestingness, of relevance is the likelihood of MTWs selecting one of the first $n$ of the Tweets in $q$. For example, consider the case of $n=2$ with a set of ten questions, $Q'$. If the MTWs selected Tweet $t^{q}_1$ or $t^{q}_2$ in five instances of $q \in Q'$, then the average likelihood of selecting one of the top $n=2$ Tweets for all $q \in Q'$ is 0.5.

Figure \ref{fig:score-dist} illustrates the relationship between increasing values of $n$ and this calculation on likelihood of selection. Although the `random' performance represents the relative likelihoods of a random selection being made when only one Tweet is selected from each question, the vast majority of questions were actually answered with only one Tweet selected. Further analysis to cover the consideration of this particular point is conducted later on in this chapter.

%Further to this, the minimum allowed value of $\gscore{t^q_1}$, which represents the highest score of all $t \in q$, was varied with the aim of demonstrating that the detection of more interesting Tweets can be more accurate when the relative score \textit{range} of a particular question is more disparate.

When considering cases where the most interesting Tweet in a particular question is, indeed, inferred as interesting ($\gscore{t} \geq 1$), then the MTWs selected one of the \textit{top two} Tweets in around 66\% of cases, and they selected one of the \textit{top three} ranked Tweets in 87\% of the questions. This demonstrates that the method's ranking ability is in agreement with humans in identifying information from the noise around them.

\begin{myobservation}
    MTWs select one of the top two highest-scoring Tweets in 66\% of the questions in $Q$ that contain at least one interesting Tweet ($\gscore{t^q_1} \geq 1$).  
\end{myobservation}


\subsubsection{Does Probability of Selection Increase with Tweet Score?}
The relationship between the likelihood of selection and a Tweet's score is also of interest. It was found that, in general, the chance of a particular MTW deciding that a Tweet, $t$, is interesting becomes greater as the value of $\gscore{t}$ increases. 

\begin{figure}[h]
\centering
\begin{tikzpicture}
\pgfplotsset{every axis plot/.style={line width=2pt}}
 \begin{axis}[
        xlabel=$ x $,
        ylabel=$P(\text{t chosen} : \gscore{t} > x)$ ,
        grid = major,
        axis y discontinuity=crunch,
        xmin=0,xmax=4,ymax=0.22,ymin=0.15
       ]
	\addplot+[smooth, mark=none]  table {5.Chapter3/data/cum-dist-score.dat};
\end{axis}
\end{tikzpicture}
\caption{Cumulative frequency representing the probability that Tweet $t$ is chosen provided that $\gscore{t}$ is greater than a given value, $x$. Note that probabilities for $\gscore{t} > 4$ have been excluded due to fewer samples}
\label{fig:score-cum-dist}
\end{figure}

Cases of Tweets where $\gscore{t} > 4$ are excluded from this analysis, for the purposes of noise reduction from fewer samples, however Figure \ref{fig:score-cum-dist} shows an observable increase in probability of selection as the score increases. This pattern is particularly evident in the score interval of [0,1], which represents the range of Tweets that the scoring scheme has inferred as uninteresting to those that achieved a correctly-predicted popularity, and are thus `as expected' in terms of interestingness. The analysis is also clear that Tweets with an inferred interestingness score of three or more are not significantly different from one another in terms of the level of interestingness assigned from the `real' human judgment.


\subsubsection{Is Interestingness More Identifiable with Greater Score Separation?}
The metrics behind the human selection in determining interesting Tweets is the final analysis conducted in this section. Of particular concern is the varying likelihood of \textit{agreement} between the MTWs and the relative properties of the Tweets and their scores in each question in different decision scenarios.

The notion of score \textit{disparity} is used to determine the difference in interest between a set of Tweets presenting with a range of different interestingness scores. To this end, each question asked of the MTWs has a disparity associated with it. The absolute score disparity, $\gdisparity{q}$, for a given ranked question, $q \in Q$, is defined as:
\[
    \gdisparity{q} = \max(\gscore{t}) - \min(\gscore{t}) \quad \forall \quad t \in q
\]

\begin{table}[h]\footnotesize
\centering
\begin{tabular}{ c | c | c | c }
	 Num. confident answers in $q$ & min. $\gdisparity{q}$ & max. $\gdisparity{q}$ & avg. $\gdisparity{q}$ \\
	 \hline
     \hline
	0 & 0 & 846 & \textbf{17.6} \\
	> 0 & 0 & 1445 & 32.1 \\
	1 & 0 & 1445 & \textbf{34.3} \\
	> 1 & 0 & 4 & 0.647 \\
	> 2 & 0 & 0.55 & 0.204\\
     \hline
\end{tabular}
\caption{Illustrating trends between the absolute $\gdisparity{q}$ with the varying number of confident answers made in $q$. Entries in \textbf{bold} are used to highlight interesting values}
\label{table:score_disparities}
\end{table}

Recall that a confident answer to a question is one where at least two of its three assessing MTWs select the same Tweet as interesting. Since an MTW could select more than one Tweet from each question, then each question may, in fact, have more than one confident answer. Table \ref{table:score_disparities} illustrates how questions with varying score disparities can have an effect on the probability of MTWs being able to make a confident decision.

The results show that the average $\gdisparity{q}$ of all $q \in Q$ is roughly double in cases where a question is answered with precisely one confident Tweet than in cases where there was no confident answer made at all, demonstrating how more interesting information is easier to identify when amongst noise. 

If several Tweets with similar scores are listed, then it becomes more difficult for an agreement to be made between the different users on which Tweet is the \textit{most} interesting. To reinforce this further, the average disparity was found to be much lower in cases where a question had multiple confident answers made. In these cases, the MTWs were unable to select one single Tweet as the most interesting and instead agreed on a set of top Tweets.

 %\begin{figure}[h]
%\centering
%\begin{tikzpicture}
%\pgfplotsset{every axis plot/.style={line width=2pt}}
% \begin{axis}[
%        xlabel=Average $\gdisparity{q}$,
%        ylabel=Cumulative probability,
%        grid = major,
%        ymax=1%,
%        ymin=0,
%        xmin=0
%       ]
%	\addplot+[mark=none]  table {5.Chapter3/data/cum-question-disparity.dat};
%\end{axis}
%\end{tikzpicture}
%\caption{Cumulative probability of a confident selection being made for question $q$ with varying $\gdisparity{q}$}
%\label{fig:cum-question-disparity}
%\end{figure}

Let $\sdisparity{q}$ be defined as the disparity of the global scores of the MTW \textit{selected} Tweets in question $q$. Table \ref{table:score-disparities-2} highlights further the effect of disparity on human selection by demonstrating that, on average, $\sdisparity{q} < \gdisparity{q} \; \forall \; q \in Q$. 

%In this case, the disparities are dependent on the particular scoring scheme declared in order to highlight the differences between the two scores, and where;
%\[
%    \ascore{t} = \frac{\gscore{t} + \uscore{t}}{2}
%\] 

This feature is particularly observable in cases where a question consists of a few Tweets having similarly high scores amongst Tweets with collectively lower scores. Therefore, inferring the interesting Tweets is easier, demonstrated by the scores of selected Tweets being generally higher, but discerning one \textit{most} interesting Tweet is not as trivial. For example, the results show that, on average, $\sdisparity{q}$ is around 57\% of $\gdisparity{q} \; \forall \; q \in Q$.

\begin{table}[h]\footnotesize
\begin{center}
\begin{tabular}{ l || c | c | c }
	 %  & $\gscore{t}$ &  $\uscore{t}$ &  $\ascore{t}$\\
      & Average $\gdisparity{q}$ & Average $\sdisparity{q}$ & Ratio \\
	 \hline
     \hline
    Average $\gscore{t}$ & 62.4 & 35.3 & 0.57 \\ 
	%$\disparity{q}$ & 62.4 & 4.7 & 33.3\\
	%$\sdisparity{q}$ & 35.3 & 3.1 & 19.0\\
	% \hline
	%Ratio & 57\% & 66\% & 58\%\\
     \hline
\end{tabular}
\end{center}
\caption{Comparing the average disparity of \textit{selected} Tweets and the disparity of \textit{all} of the Tweets in questions in $Q$}
\label{table:score-disparities-2}
\end{table}


\subsection{Methodology and Validation Remarks}
In this section, the proposed improvements to the inference methodology have been implemented and assessed under a randomised controlled trial using Amazon's Mechanical Turk to crowdsource the validations.

Results from the analyses indicate the method's relative advantages over the techniques used in the previous chapter. In particular, the new method is applicable to generating appropriate interestingness inferences for Tweets from all users on Twitter, is capable of effectively \textit{ranking} Tweets in order of interestingness, and is far more efficient in model training. It is also much more readily supportive of `on demand' inferences.

However, the crowdsourcing validations conducted were contributed to by people who shared no connection with the authors of the Tweets, and were thus assessing Tweets from outside of their own local network. As such, these evaluations are likely made on the basis of `global interestingness', where Tweets that convey some meaning are highlighted from the noise, yet where the `interesting' Tweets selected may not actually be relevant to the assessing MTWs. It is known, however, that Twitter users typically form followships between other users that produce information of both interest and relevance.

The following section addresses this by evaluating the accuracy of the scoring mechanism when users assess Tweets from within their own local network. This is of concern to the research in this Thesis since the users have a pre-defined interest in the authors of the Tweets they are evaluating. 


\section{Addressing Individual Information Relevance}
In these analyses, results are studied from validations conducted through users assessing Tweets existing within their own local network. In particular, the interestingness scoring methodology will be validated against people's Tweet rankings for those users that they directly follow. Interactions with Twitter in this section relate to v1.1 of the Twitter API, as this research was conducted after the mandatory switch-over to this version.

Through assessment in this way, the Tweets being assessed are more relevant to their `environments', which, in this case, consist of those users who would naturally also receive these Tweets and who are making the interestingness decisions based on their content.


\subsection{Methodology}
As will become clear, no initial data collection is required for these analyses. Instead, users contributing to the crowdsourced analysis interacted almost directly with Twitter during the course of their assessments, which involved the studying of Tweets sent from the friends of the assessing user.

For this purpose, a web application was set up in July 2013 and ran until August 2013, which allowed visiting users to `sign in' using their Twitter account through OAuth. As with v1, v1.1 of Twitter's REST API directly supports this kind of behaviour, and provides the authenticated application with access keys enabling it to interface with the API on the authenticating user's behalf. Applications registered on Twitter can have different levels of access to users' accounts - from read-only, in which Tweets, follower information, and so on, can be retrieved; to read and write, with which new Tweets can be posted for the user and new followships can be created. An advantage of using OAuth in this fashion is that each user has a separate rate limit associated with it, meaning that the application could retrieve a lot of information, if necessary, yet without exceeding the rate limit afforded to application-only authentication by the new policies of v1.1 of the API.

In this case, the web application was advertised through word of mouth and through OSNs, such as Facebook and Twitter (see Figure \ref{fig:organic_advertising}) itself, as well as through Mechanical Turk. In the latter case, a special link to the site was provided to MTWs, and a code was displayed to them on completion of the task, which they could enter into Mechanical Turk in order to be paid. Participants contributing from the word of mouth and OSN categories are termed as `organic' participants. Since the analysis depended on users assessing Tweets from their Twitter friends, participants could only take part if they had a Twitter account with at least 30 friends.

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{5.Chapter3/Media/organic_advertising.png} 
\caption{Advertising the validation site on Twitter.}
\label{fig:organic_advertising}
\end{figure}

After signing into the read-only application\footnote{Twinterest: source available at https://github.com/flyingsparx/twinterestingness} and beginning the procedure, participants were faced with a series of ten Tweet timelines. The first, illustrated by Figure \ref{fig:twinterest}, consisted of the most recent 20 Tweets from the participant's home timeline, and the next nine consisted of user timelines of the participant's friends. Although the selection of friends for the nine user timelines was performed at random, a slight bias was applied towards selecting friends with a higher follower count through a weighted roulette-wheel selection.  Due to the nature of scale-free graphs, there are many vertices with few edges, and few with many edges. As such, in order to obtain a more even distribution of user influence, the weighting was necessary to ensure that the scoring mechanism could be validated against a range of users expressing Tweets with a wider variety of retweet counts.

\begin{figure}[h]
\centering
\includegraphics[scale=0.35]{5.Chapter3/Media/twinterest.png} 
\caption{Screenshot of the experimental web application.}
\label{fig:twinterest}
\end{figure}

Participants were simply asked to select the Tweets that they found to be interesting from each of the timelines, as illustrated by Figure \ref{fig:twinterest}, and were not permitted to proceed to the next timeline without selecting at least one Tweet. Note that, at this stage, the Tweets being assessed did not have interestingness scores applied to them. A Tweet in a timeline that was selected was considered to be interesting, and those not selected were considered non-interesting.


\subsection{Assigning Scores to the Assessed Tweets}
A total of 580 timelines were assessed through the application validations, consisting of 389 contributed to by MTWs and 191 from organic participants. The totals are not precisely divisible by ten since not all participants assessed all of their ten timelines before leaving the application, but no single participant contributed more than ten timeline assessments. In this case, all responses were considered as confident since it was not appropriate under the conditions of the validation test to gain more than one assessment for each Tweet. Although there was likely some friend overlap between the participants, this was not necessarily the case in the vast majority of users assessed.  

\begin{figure}[h]
\centering
\begin{tikzpicture}
 \begin{axis}[
        xlabel=Number of selected Tweets,
        ylabel=Frequency,
        grid = major,
        ymin=0,
        xmin=0,
        xmax=9,
        ybar
       ]
	\addplot coordinates {
        % probabilities: (1,0.460992908)(2,0.241134752)(3,0.19858156)(4,0.04964539)(5,0.021276596)(6,0.007092199)(7,0.007092199)(8,0.007092199)(9,0.007092199)
        (1,267.37588664)(2,139.85815616)(3,115.1773048)(4,28.7943262)(5,12.34042568)(6,4.11347542)(7,4.11347542)(8,4.11347542)(9,4.11347542)
    };
\end{axis}
\end{tikzpicture}
\caption{Frequency distribution of the number of Tweets selected from each timelines by the participants}
\label{fig:num_tweets_selected}
\end{figure}

The validation test resulted in a set of just under 10,000 Tweets, authored by 936 unique users, that participants had made interestingness decisions on. Let $\testsetb{}$ be the set containing these Tweets, and in order to determine their predicted expected retweet counts as part of assigning them scores, two procedures were required to take place;
\begin{itemize}
    \item Collect further data on each assessed \textit{author} in order to generate the `user' models, and;
    \item Collect further data on each assessed \textit{Tweet} in order to classify it against the previously-generated global model and the relevant user model.
\end{itemize}
The global model used was the same large model generated during the previous validation tests.

For reasons of privacy, each participant's Twitter API credentials were not maintained by the application and so standard authenticated REST API requests were performed to collect the additional data required. In particular, in August 2013, each of the 936 users  $\left\{ \aut{t}{O} \; \forall \; t \in \testsetb{} \right\}$ were queried under an identical collection scheme to that used as part of the previous validation; information on the author itself and on a sample of the author's followers and friends was retrieved. The collected information was also assigned to each of that user's Tweets $t' \in \testsetb{}$ so that an instance could be built for every $t \in \testsetb{}$ according to the features described in Table \ref{table:globalfeatures}. These Tweets were then classified by the global model and their appropriate user model, which was built from its author's features, in order to eventually produce the two scores. 

It should be noted that if a particular user follows another whose account is protected (see earlier in the thesis for further information on this), then the former user's API credentials can be used to view the latter's information and Tweets. However, since, during the data-collection, a static account was used to query the API, then Tweets and user information for accounts that are protected could not successfully be retrieved. This means that user and Tweet data for these users could not be collected for the purposes of training the user model and testing Tweets against this and the global model, and thus Tweets from protected authors had to be removed from $\testsetb{}$. The numbers stated in this section are those of the \textit{final} dataset after removing these Tweets and users.


\subsection{Results from the Further Validations}
In this section, the patterns observed through the comparison of the Tweets inferred as interesting through the scores and those indicated as interesting by the human participants are analysed. The combination of both sets of participants was considered in the following analyses. As before, the $\gscore{t}$ was used as the scoring scheme for the analysis in this section.

\subsubsection{Performance of timeline ranking}

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Chance of occurrence,
            xlabel=$n$,
            grid=major,
            xmin=0,
            xmax=20,
            ymin=0,
            ymax=1
            ]
       \addplot[mark=+,only marks,blue] plot coordinates{
            (0,0)(1,0.194737)(2,0.310526)(3,0.373684)(4,0.484211)(5,0.557895)(6,0.636842)(7,0.694737)(8,0.747368)(9,0.784211)(10,0.847368)(11,0.878947)(12,0.915789)(13,0.931579)(14,0.947368)(15,0.963158)(16,0.989474)(17,0.989474)(18,1)(19,1)(20,1)
        };    
        \addplot[gray] plot coordinates{
            (0,0)(1,0.05)(2,0.1)(3,0.15)(4,0.2)(5,0.25)(6,0.3)(7,0.35)(8,0.4)(9,0.45)(10,0.5)(11,0.55)(12,0.6)(13,0.65)(14,0.7)(15,0.75)(16,0.8)(17,0.85)(18,0.9)(19,0.95)(20,1) 
        }; 
    \end{axis}
    \end{tikzpicture}
    \caption{In timelines where one Tweet was selected}
    \label{fig:rank-one}
\end{subfigure}
\quad
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Chance of occurrence,
            xlabel=$n$,
            grid=major,
            xmin=0,
            xmax=20,
            ymin=0,
            ymax=1
            ]
       \addplot[mark=+,only marks,blue] plot coordinates{
       (0,0)(1,0.275862)(2,0.405172)(3,0.543103)(4,0.637931)(5,0.715517)(6,0.758621)(7,0.810345)(8,0.836207)(9,0.87069)(10,0.887931)(11,0.913793)(12,0.939655)(13,0.948276)(14,0.948276)(15,0.956897)(16,0.974138)(17,0.982759)(18,0.991379)(19,1)(20,1) 
        };    
        \addplot[gray] plot coordinates{
        (0,0)(1,0.1)(2,0.194736842)(3,0.284210526)(4,0.368421053)(5,0.447368421)(6,0.521052632)(7,0.589473684)(8,0.652631579)(9,0.710526316)(10,0.763157895)(11,0.810526316)(12,0.852631579)(13,0.889473684)(14,0.921052632)(15,0.947368421)(16,0.968421053)(17,0.984210526)(18,0.994736842)(19,1)(20,1)
        }; 
    \end{axis}
    \end{tikzpicture}
    \caption{In timelines where two Tweets were selected}
    \label{fig:rank-two}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Chance of occurrence,
            xlabel=$n$,
            grid=major,
            xmin=0,
            xmax=20,
            ymin=0,
            ymax=1,
            legend entries={Scoring methodology performance, Random selection},
            legend style={at={(2.4,0.5)}, nodes={right}}
            ]
       \addplot[mark=+,only marks,blue] plot coordinates{
            (0,0)(1,0.282353)(2,0.447059)(3,0.517647)(4,0.682353)(5,0.717647)(6,0.776471)(7,0.823529)(8,0.858824)(9,0.882353)(10,0.905882)(11,0.941176)(12,0.952941)(13,0.976471)(14,0.988235)(15,1)(16,1)(17,1)(18,1)(19,1)(20,1)
        };    
        \addplot[gray] plot coordinates{
            (0,0)(1,0.15)(2,0.284210526)(3,0.403508772)(4,0.50877193)(5,0.600877193)(6,0.680701754)(7,0.749122807)(8,0.807017544)(9,0.855263158)(10,0.894736842)(11,0.926315789)(12,0.950877193)(13,0.969298246)(14,0.98245614)(15,0.99122807)(16,0.996491228)(17,0.999122807)(18,1)(19,1)(20,1)
        }; 
    \end{axis}
    \end{tikzpicture}
    \caption{In timelines where three Tweets were selected}
    \label{fig:rank-three}
\end{subfigure}
\caption{The chance of a participant selecting one of the \textit{highest} $n$ ranked Tweets in the timeline}
\label{fig:ranked_timelines_1}
\end{figure}

In the previous validation, the performance of the interestingness scores in ranking Tweets was assessed on a per-question basis. The same concept is expanded here to apply a similar assessment of the scores on the present validation test.

In this case, each assessed \textit{timeline} was ranked in order of descending interestingness score in an effort to find the probability of a participant selecting a Tweet occurring in the top $n$ of Tweets. Timelines were up to 20 Tweets long, compared to the five used in the Mechanical Turk questions in the initial validation test, but Figure \ref{fig:ranked_timelines_1} demonstrates that the mechanism is still able to effectively rank the Tweets. Since the timelines are larger than the questions used before, the chance of a participant selecting multiple Tweets from a timeline was greater, as indicated by Figure \ref{fig:num_tweets_selected}. To illustrate this, the results for this analysis are demonstrated against the appropriate random performance benchmark produced by the different selection criteria.

For Figure \ref{fig:ranked_timelines_1}(\subref{fig:rank-one}), the random performance is defined as the probability of a Tweet randomly selected from a timeline being in the top $n$ of ranked Tweets in the timeline. In Figures \ref{fig:ranked_timelines_1}(\subref{fig:rank-two}) and \ref{fig:ranked_timelines_1}(\subref{fig:rank-three}), it represents the respective probabilities of the random selection of two and three Tweets being in the top $n$ of ranked Tweets.

It is clear that the scores are able to identify interesting information from the noise around it, and so further analyses were conducted into the performance of the scores in detecting \textit{un}-interesting information. In this scenario, each timeline had its Tweets ranked in order of \textit{ascending} interestingness score and calculations were carried out into the probability of participants \textit{not} selecting the \textit{bottom} $n$ interesting Tweets in each timeline.

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Chance of occurrence,
            xlabel=$n$,
            grid=major,
            xmin=0,
            xmax=20,
            ymin=0,
            ymax=1
            ]
       \addplot[mark=+,only marks,blue] plot coordinates{
            (0,1)(1,0.908571429)(2,0.828571429)(3,0.742857143)(4,0.662857143)(5,0.582857143)(6,0.564417178)(7,0.5)(8,0.5)(9,0.488188976)(10,0.398305085)(11,0.339449541)(12,0.347368421)(13,0.337349398)(14,0.260273973)(15,0.21875)(16,0.244897959)(17,0.189189189)(18,0.107142857)(19,0.052631579)(20,0)
        };    
        \addplot[gray] plot coordinates{
           (0,1)(1,0.95)(2,0.9)(3,0.85)(4,0.8)(5,0.75)(6,0.7)(7,0.65)(8,0.6)(9,0.55)(10,0.5)(11,0.45)(12,0.4)(13,0.35)(14,0.3)(15,0.25)(16,0.2)(17,0.15)(18,0.1)(19,0.05)(20,0) 
        }; 
    \end{axis}
    \end{tikzpicture}
    \caption{In timelines where one Tweet was selected}
    \label{fig:rank-one2}
\end{subfigure}
\quad
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Chance of occurrence,
            xlabel=$n$,
            grid=major,
            xmin=0,
            xmax=20,
            ymin=0,
            ymax=1
            ]
       \addplot[mark=+,only marks,blue] plot coordinates{
            (0,1)(1,0.854545455)(2,0.745454545)(3,0.645454545)(4,0.545454545)(5,0.427272727)(6,0.38317757)(7,0.336734694)(8,0.276595745)(9,0.224719101)(10,0.159090909)(11,0.111111111)(12,0.130434783)(13,0.064516129)(14,0.055555556)(15,0.020408163)(16,0.024390244)(17,0)(18,0)(19,0)(20,0) 
        };    
        \addplot[gray] plot coordinates{
            (0,1)(1,0.9)(2,0.805263158)(3,0.715789474)(4,0.631578947)(5,0.552631579)(6,0.478947368)(7,0.410526316)(8,0.347368421)(9,0.289473684)(10,0.236842105)(11,0.189473684)(12,0.147368421)(13,0.110526316)(14,0.078947368)(15,0.052631579)(16,0.031578947)(17,0.015789474)(18,0.005263158)(19,0)(20,0)
        }; 
    \end{axis}
    \end{tikzpicture}
    \caption{In timelines where two Tweets were selected}
    \label{fig:rank-two2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Chance of occurrence,
            xlabel=$n$,
            grid=major,
            xmin=0,
            xmax=20,
            ymin=0,
            ymax=1,
            legend entries={Scoring methodology performance, Random performance},
            legend style={at={(2.4,0.5)}, nodes={right}}
            ]
       \addplot[mark=+,only marks,blue] plot coordinates{
            (0,1)(1,0.773809524)(2,0.535714286)(3,0.392857143)(4,0.297619048)(5,0.261904762)(6,0.2375)(7,0.151898734)(8,0.12)(9,0.108108108)(10,0.123076923)(11,0.079365079)(12,0.054545455)(13,0.037735849)(14,0)(15,0)(16,0)(17,0)(18,0)(19,0)(20,0)
        };    
        \addplot[gray] plot coordinates{
            (0,1)(1,0.85)(2,0.715789474)(3,0.596491228)(4,0.49122807)(5,0.399122807)(6,0.319298246)(7,0.250877193)(8,0.192982456)(9,0.144736842)(10,0.105263158)(11,0.073684211)(12,0.049122807)(13,0.030701754)(14,0.01754386)(15,0.00877193)(16,0.003508772)(17,0.000877193)(18,0)(19,0)(20,0)
        }; 
    \end{axis}
    \end{tikzpicture}
    \caption{In timelines where three Tweets were selected}
    \label{fig:rank-three2}
\end{subfigure}
\caption{The chance of a participant \textit{not} selecting one of the \textit{lowest} $n$ ranked Tweets in the timeline}
\label{fig:ranked_timelines_2}
\end{figure}

Figure \ref{fig:ranked_timelines_2} illustrates this data, again with the different selection criteria. It is clear that although the scores are able to assist in identifying non-interesting information, the difference between this performance and the random selection case is not as great as with detecting the positively interesting Tweets.



\subsubsection{Precision \& Recall}

\begin{figure}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Accuracy,
            xlabel=$h$,
            grid=major,
            xmin=0,
            xmax=20,
            ymin=0,
            ymax=1,
            legend entries={Precision, Recall},
            ]
       \addplot[mark=+,only marks,blue] plot coordinates{
            (1, 0.213521695257 ) (2, 0.218706375058 ) (3, 0.22301064369 ) (4, 0.227458458186 ) (5, 0.227862705509 ) (6, 0.230487804878 ) (7, 0.231948881789 ) (8, 0.232511658894 ) (9, 0.234401930369 ) (10, 0.234302116972 ) (11, 0.236075248986 ) (12, 0.236074270557 ) (13, 0.234216679657 ) (14, 0.231907237105 ) (15, 0.231145536078 ) (16, 0.229870671673 ) (17, 0.231426131512 ) (18, 0.23231441048 ) (19, 0.233035714286 ) (20, 0.235079726651 )
        };    
        \addplot[mark=+,only marks,red] plot coordinates{
            (1, 0.693315858453 ) (2, 0.615989515072 ) (3, 0.576671035387 ) (4, 0.547182175623 ) (5, 0.517693315858 ) (6, 0.495412844037 ) (7, 0.475753604194 ) (8, 0.457404980341 ) (9, 0.445609436435 ) (10, 0.427916120577 ) (11, 0.419397116645 ) (12, 0.408256880734 ) (13, 0.393840104849 ) (14, 0.380078636959 ) (15, 0.371559633028 ) (16, 0.361074705111 ) (17, 0.355176933159 ) (18, 0.348623853211 ) (19, 0.342070773263 ) (20, 0.338138925295 )
        };
    \end{axis}
    \end{tikzpicture}
    \caption{Accuracy (in terms of precision \& recall) on the scoring mechanism with varying score threshold, $h$}
    \label{fig:precision_recall_twinterest}
\end{figure}

Precision and recall have been useful ways of verifying the relative performance of the binning scheme for the simulation algorithm, in assessing the qualities of the various potential classifiers, and in several pieces of literature carrying out social network analysis, as highlighted in the Background chapter.

The two metrics are also useful in demonstrating the performance of the scoring mechanism with respect to a varying \textit{interestingness threshold}. Of concern is the similarity between the interestingness inferences made by the methodology and the interestingness decisions made by the participants. $h$ is defined as an interestingness threshold, where a particular Tweet, $t$, is inferred as interesting only if $\gscore{t} \geq h$. Precision and recall use a varying $h$ value to determine the accuracy of the inference methodology, where;
\[
    \textrm{Precision} = \frac{\textrm{Of Tweets with score} \geq h\textrm{, number selected by participants}}{\textrm{Number of Tweets with score} \geq h}
\]  
\[
    \textrm{Recall} = \frac{\textrm{Of Tweets selected by participants, number with score} \geq h}{\textrm{Number of Tweets selected by participants}}
\]  

Figure \ref{fig:precision_recall_twinterest} illustrates the precision and recall for $\testsetb{}$ for $1 \geq h \geq 20$. As $h$ increases, the number of Tweets with $\gscore{t} \geq h$ decreases, causing the recall value to reduce. Although the precision increases slightly with greater $h$ values (due to a smaller number of Tweets with higher scores), it is mostly consistent between 0.23 and 0.27 in the range shown. 

Since the recall is more variable, a value of $h < 5$ provides the best threshold for making a collective interestingness inference, as this provides a reasonable recall and precision in the context of the data. 


\subsubsection{Crowdsourced timeline selections}

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
     \begin{axis}[
        view={0}{90},
        ylabel=Position of selection,
        colormap/hot,
        grid=none,
        y dir=reverse,
        xticklabels=none,
        xticklabel style=none,
        yticklabel style=none,
        ticks=none,
        separate axis lines,
        y axis line style= { draw opacity=0 },
        x axis line style= { draw opacity=0 },
        width=8cm,
        height=8cm,
        draw=none]
      \addplot3[surf] table [row sep=newline] {5.Chapter3/data/heatmap.dat};
      \addplot[black] plot coordinates{       
            (0,6.0743)(1,6.0743)
      };
     \end{axis}
    \end{tikzpicture}
    \caption{Selections made by organic participants}
\end{subfigure}
\quad
\begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.8]
     \begin{axis}[
        view={0}{90},
        ylabel=Position of selection,
        colormap/hot,
        grid=none,
        y dir=reverse,
        xticklabels=none,
        xticklabel style=none,
        yticklabel style=none,
        ticks=none,
        separate axis lines,
        y axis line style= { draw opacity=0 },
        x axis line style= { draw opacity=0 },
        width=8cm,
        height=8cm,
        draw=none]
      \addplot3[surf] table [row sep=newline] {5.Chapter3/data/heatmap2.dat};
      \addplot[black] plot coordinates{       
            (0,5.825323475)(1,5.825323475)
      };
     \end{axis}
    \end{tikzpicture}
    \caption{Selections made by MTWs}
\end{subfigure}
\caption{Heatmaps illustrating the timeline position of Tweet selections made by participants. Mean selection position is indicated}
\label{fig:selection_heatmaps}
\end{figure}

A brief study was additionally made into the selections of Tweets made by the participants. Of particular interest is the \textit{difference} in performance between those participants that were paid to take part (the MTWs) and those who took part without being paid (the organic participants), and whether one group was more likely to select Tweets near the top of the timeline without scrolling down to read and select those at the bottom of the timeline. Reasons for this case could be laziness on the behalf of the participant, or simply for speed.

Figure \ref{fig:selection_heatmaps} shows the results of this study, and revealed that there was very little difference between the two participant groups. The organic participants, on average, selected the Tweet at position 6.07 in the timeline, and the MTWs selected Tweets at the average position of 5.83 out of 20 maximum available positions. Whilst these selection position averages are both relatively near to the top of the timeline, it should be noted that the \textit{mean} timeline length was of 14 Tweets, and thus purely average random selections would be made at around the mark of the seventh Tweet.

It is felt, therefore, that there was some bias in both participant groups in that they were both slightly more likely to select Tweets nearer the top of the timeline than scroll down to view, and make interestingness judgments on those, nearer the bottom of the timelines. As with the previous validation tests, it was also possible to demonstrate that the score disparity between Tweets in a particular timeline is greater in cases where only one selection is made by the participants (Figure \ref{fig:disparity2}). 

\begin{figure}
    \centering
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            ylabel=Maximum disparity,
            xlabel=Number of selected Tweets,
            grid=major,
            xmin=1,
            xmax=9,
            ymin=0
            ]
        \addplot[mark=+,only marks,blue] plot coordinates{
            (1,4246.666667)(2,2694.0)(3,2444.0)(4,125.0)(5,29.0)(6,998.6666667)(7,15.0)(8,19.33333333)(9,40.33333333)
        }; 
    \end{axis}
    \end{tikzpicture}
\caption{Relationship between the number of selected Tweets in a timeline and the maximum score disparity of the timeline}
\label{fig:disparity2}
\end{figure}


\section{Chapter Summary}
In this chapter, an improvement over the previous iteration of the Tweet interestingness inference methodology has been introduced, tested, and analysed. Bringing the research into the social structure of Twitter forward from the previous chapter, it was possible to determine areas for improvement and the useful metrics for governing the selection of new features. 

Question \textbf{RQ4} from the initial hypotheses has been answered in order to show that interestingness of Tweets can be inferred non-semantically with some degree of accuracy, and the methods have been able to demonstrate their ability to rank Tweets in order of interest. It is clear how the method could be used for highlighting particularly outstanding Tweets in order to, for example, identify the more controversial Tweets surrounding a particular event, and for producing a digest of Tweet `timelines' based on estimated interest rather than time.

\subsection{Interestingness Scores}
The new methodology introduced the notion of scores, which can be assigned to Tweets in order to signify their relative interestingness. These scores are based on the ratio between the popularity of a Tweet, measured by its observed retweet count, and a value representing an \textit{expected} retweet count for the Tweet. The scoring mechanism works such that different types of users, including those ranging across influence and activity frequency\footnote{The activity frequency is the rate at which a user posts Tweets.} levels, can have their Tweets assessed on the same scale. 

Two scoring schemes were set up, which are derived from distinct methods for generating the expected retweet count. One method is based on comparing a Tweet's (and its author's) features to a \textit{global} model trained on a large number of Tweets collected from Twitter. The other is generated through the comparison of the Tweet's features to a \textit{user} model trained only on other Tweets posted by that same particular user. Generally, there was found to be a non-significant difference between the performance of the two scores, however, and thus they were both used interchangeably during the validations.

\subsection{Methodology Validations}
Two sets of validations were conducted into verifying the performance and accuracy of the new methodology and the scores it produced - one in which Tweets were placed into questions on Amazon's Mechanical Turk, in which MTWs were asked to select the most interesting Tweets; and another, in which participants were asked to sign-in through Twitter and then assess Tweets from users they actually follow.

In the first case, the participants shared no connection with the authors of the Tweets they were assessing (except in the case of coincidences), and were therefore assessing Tweets on a \textit{global} interest level. In particular, this largely involves determining the interesting information from the noise around it. In the second set, information \textit{relevance} came more into play, since participants were assessing Tweets from users they have already declared an interest in (through the action of following).

In both test cases, the validations showed the scores to be able to appropriately label Tweets according to interestingness in a variety of different ways. The second test included an analysis demonstrating that the scores are more efficient at determining interesting Tweets than \textit{un}-interesting Tweets, the latter of which would be useful in deciding on a set of Tweets to discard from an interesting set. 


\subsection{Improvements and Qualities}
The newly introduced methodology presents several improvements over that described in the previous chapter. In particular, the performance of the scores have shown a large accuracy improvement in determining interesting information. The previous method also did not take into account \textit{how} interesting a particular Tweet may be, and was only able to make a binary interesting/uninteresting decision for each Tweet.

Another large improvement is the ability of the scoring method to be applied to a much wider range of Tweets. The previous method was realistically unable to assess Tweets from users with more than 300 or so followers due to data collection inefficiency, the time taken, and the computational complexity involved in simulating large user graphs. The new method can be used to assign scores to Tweets in a more ``on demand'' fashion, where only a small amount of information for each Tweet is required in order to generate the features needed to predict the estimated retweet counts. The scores also allow Tweets from many different sources to be assessed on the same scoring scale, meaning that Tweets on a mixed timeline can be appropriately compared to one-another, as demonstrated by the second set of validations.
